{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('AttemptUsingImageGenerator/')\n",
    "\n",
    "import intelliscope\n",
    "from intelliscope import SelectDataset\n",
    "from ml_utils import log_model_weights,log_epoch_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from sklearn.utils import shuffle\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Activation\n",
    "from tensorflow.keras.layers import Conv2D, MaxPool2D\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\n",
    "from tensorflow.keras import regularizers\n",
    "print(tf.__version__)\n",
    "import neptune\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ubuntu/intelliscope/notebooks/physionet2016/pcg_ecg/Ramith/regular'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['NEPTUNE_API_TOKEN']=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vdWkubmVwdHVuZS5haSIsImFwaV91cmwiOiJodHRwczovL3VpLm5lcHR1bmUuYWkiLCJhcGlfa2V5IjoiNjBlODllY2ItMDcyZC00OGNmLThjZWYtZjM2MjE4Y2M5ZDM0In0=\"\n",
    "os.environ['NEPTUNE_PROJECT']=\"icassp/Hybrid'\"\n",
    "#os.environ['NEPTUNE_NOTEBOOK_ID']=\"4897a223-8a9f-4f00-8cec-22eda690858d\"\n",
    "os.environ['NEPTUNE_NOTEBOOK_PATH']=\"notebooks_ramith/CNN_hybrid_transfer_learning.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading... >  ICASSP_01\n",
      "\t=> /home/ubuntu/intelliscope/datasets/PhysioNet2016/preprocessed/training-a/original/ecg/01-cmor1.5-1.0/\n",
      "\t=> /home/ubuntu/intelliscope/datasets/PhysioNet2016/preprocessed/training-a/original/pcg/01-morl/\n"
     ]
    }
   ],
   "source": [
    "NEPTUNE_API_TOKEN=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vdWkubmVwdHVuZS5haSIsImFwaV91cmwiOiJodHRwczovL3VpLm5lcHR1bmUuYWkiLCJhcGlfa2V5IjoiNjBlODllY2ItMDcyZC00OGNmLThjZWYtZjM2MjE4Y2M5ZDM0In0=\"\n",
    "PARAMS = {'ignore_blanks' : True,\n",
    "          'epochs' : 15,\n",
    "         'validation_split':0.2,\n",
    "         'batch_size' : 20,\n",
    "         'train_test_split':0.8,\n",
    "         'dense_layers':80,\n",
    "          'dropout':0.1,\n",
    "         'dataset_path_ecg': '',\n",
    "         'dataset_path_pcg': '',\n",
    "         'image_type' : 'rgb',\n",
    "         'im_type':6,\n",
    "         'Read_Type':1,\n",
    "          'opt':'adam',\n",
    "          'data_balanced':False,\n",
    "         }\n",
    "\n",
    "PARAMS['dataset_path_pcg'],PARAMS['dataset_path_ecg'] = SelectDataset(0)\n",
    "\n",
    "m = PARAMS['Read_Type']*3 + int(PARAMS['Read_Type']==0)\n",
    "e = PARAMS['Read_Type']*6 + int(PARAMS['Read_Type']==0)*2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_dir = [PARAMS['dataset_path_pcg'][:-1]+'_train/',PARAMS['dataset_path_ecg'][:-1]+'_train/']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2254 images belonging to 2 classes.\n",
      "Found 562 images belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/core-env/lib/python3.6/site-packages/keras_preprocessing/image/utils.py:179: UserWarning: Using \".tiff\" files with multiple bands will cause distortion. Please verify your output.\n",
      "  warnings.warn('Using \".tiff\" files with multiple bands '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2254 images belonging to 2 classes.\n",
      "Found 562 images belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: There is a new version of neptune-client 0.4.125 (installed: 0.4.124).\n",
      "NVMLError: NVML Shared Library Not Found - GPU usage metrics may not be reported.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://ui.neptune.ai/icassp/Hybrid/e/HYBRID-91\n"
     ]
    }
   ],
   "source": [
    "k1,k2 = 272, 462\n",
    "s_x = 82;\n",
    "e_x = 544;\n",
    "s_y = 43;\n",
    "e_y = 315;\n",
    "\n",
    "batch_size=PARAMS['batch_size']\n",
    "train_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2) # set validation split\n",
    "\n",
    "def cropper(img):\n",
    "    # Note: image_data_format is 'channel_last'\n",
    "    assert img.shape[2] == 3\n",
    "    #print(img.shape)\n",
    "    #plt.figure()\n",
    "    #wplt.imshow(img[s_y:e_y,s_x:e_x])\n",
    "    return img[s_y:e_y,s_x:e_x]\n",
    "def generate_generator_multiple(generator,dir1, dir2, batch_size,subset):\n",
    "    #k1, k2=360, 648\n",
    "    k1, k2=272, 462\n",
    "    genX1 = generator.flow_from_directory(dir1,\n",
    "                                          target_size=(360, 648),\n",
    "                                          class_mode = 'categorical',\n",
    "                                          batch_size = batch_size,\n",
    "                                          subset=subset,\n",
    "                                          shuffle=False, \n",
    "                                          seed=7)\n",
    "    \n",
    "    genX2 = generator.flow_from_directory(dir2,\n",
    "                                          target_size=(360, 648),\n",
    "                                          class_mode = 'categorical',\n",
    "                                          batch_size = batch_size,\n",
    "                                          subset=subset,\n",
    "                                          shuffle=False, \n",
    "                                         seed=7)\n",
    "    while True:\n",
    "            X1i, y1i = genX1.next()\n",
    "            X2i, y2i = genX2.next()\n",
    "            \n",
    "            X1i_cropped = np.zeros((X1i.shape[0], k1, k2, 3))\n",
    "            X2i_cropped = np.zeros((X2i.shape[0], k1, k2, 3))\n",
    "\n",
    "            for i in range(X1i.shape[0]):      \n",
    "                X1i_cropped[i] = cropper(X1i[i])\n",
    "            for i in range(X2i.shape[0]):      \n",
    "                X2i_cropped[i] = cropper(X2i[i])\n",
    "                \n",
    "            yield [X1i_cropped, X2i_cropped], y1i  #Yield both images and their mutual label\n",
    "    \n",
    "    \n",
    "dual_training   = generate_generator_multiple(train_datagen,train_data_dir[0],train_data_dir[1],batch_size,'training')\n",
    "dual_validation = generate_generator_multiple(train_datagen,train_data_dir[0],train_data_dir[1],batch_size,'validation')\n",
    "\n",
    "pcg_training=train_datagen.flow_from_directory(train_data_dir[0],target_size=(k1,k2),class_mode = 'categorical', batch_size = batch_size, subset='training', shuffle=False,  seed=7)\n",
    "pcg_validation=train_datagen.flow_from_directory(train_data_dir[0],target_size=(k1,k2),class_mode = 'categorical', batch_size = batch_size, subset='validation', shuffle=False,  seed=7)\n",
    "\n",
    "ecg_training=train_datagen.flow_from_directory(train_data_dir[1],target_size=(k1,k2),class_mode = 'categorical', batch_size = batch_size, subset='training', shuffle=False,  seed=7)\n",
    "ecg_validation=train_datagen.flow_from_directory(train_data_dir[1],target_size=(k1,k2),class_mode = 'categorical', batch_size = batch_size, subset='validation', shuffle=False,  seed=7)\n",
    "\n",
    "neptune.init('icassp/Hybrid',NEPTUNE_API_TOKEN)\n",
    "exp = neptune.create_experiment(name='CNN_hybrid_udith_test',params=PARAMS,upload_source_files=['exp.ipynb'],tags=['HS','ECG','ICASSP', 'PCG_only'],upload_stdout=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:The given value for groups will be overwritten.\n",
      "WARNING:root:The given value for groups will be overwritten.\n",
      "WARNING:root:The given value for groups will be overwritten.\n",
      "WARNING:root:The given value for groups will be overwritten.\n",
      "WARNING:root:The given value for groups will be overwritten.\n",
      "WARNING:root:The given value for groups will be overwritten.\n",
      "WARNING:root:The given value for groups will be overwritten.\n",
      "WARNING:root:The given value for groups will be overwritten.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCG\n",
      "ECG\n"
     ]
    }
   ],
   "source": [
    "model_pcg = tf.keras.models.load_model('/home/ubuntu/intelliscope/notebooks/physionet2016/pcg/models/Experiment(PCG-50).h5')\n",
    "model_ecg = tf.keras.models.load_model('/home/ubuntu/intelliscope/notebooks/physionet2017/ecg/models/Experiment(ECG-24).h5')\n",
    "\n",
    "print('PCG')\n",
    "#model_pcg.summary()\n",
    "print('ECG')\n",
    "#model_ecg.summary()\n",
    "\n",
    "# warning due to Instance Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 3840)\n",
      "(None, 3840)\n"
     ]
    }
   ],
   "source": [
    "pcg_layer='flatten_1'\n",
    "ecg_layer='flatten'\n",
    "\n",
    "HS_MODEL= tf.keras.Model(inputs=model_pcg.input, outputs=model_pcg.get_layer(pcg_layer).output)\n",
    "ECG_MODEL= tf.keras.Model(inputs=model_ecg.input, outputs=model_ecg.get_layer(ecg_layer).output)\n",
    "HS_MODEL.trainable=True\n",
    "ECG_MODEL.trainable=True\n",
    "\n",
    "hs_input = tf.keras.layers.Input(shape=(272,462,m))\n",
    "ecg_input = tf.keras.layers.Input(shape=(272,462,m))\n",
    "\n",
    "hs_model = HS_MODEL(hs_input)\n",
    "ecg_model = ECG_MODEL(ecg_input)\n",
    "\n",
    "print(ecg_model.get_shape())\n",
    "print(hs_model.get_shape())\n",
    "\n",
    "hs_model_ = tf.keras.layers.LayerNormalization()(hs_model)\n",
    "ecg_model_ = tf.keras.layers.LayerNormalization()(ecg_model)\n",
    "concat     = tf.keras.layers.concatenate([hs_model_, ecg_model_])\n",
    "dropout    = tf.keras.layers.Dropout(PARAMS['dropout'])(concat)\n",
    "dense      = tf.keras.layers.Dense(PARAMS['dense_layers'], activation='relu')(dropout)\n",
    "dense_1    = tf.keras.layers.Dense(20, activation='relu')(dense)\n",
    "output = tf.keras.layers.Dense(2, activation='softmax')(dense_1)\n",
    "\n",
    "model_hybrid = tf.keras.Model(inputs=[hs_input, ecg_input], outputs=[output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_weights(model):\n",
    "    session = K.get_session()\n",
    "    for layer in model.layers:\n",
    "        if isinstance(layer, Dense):\n",
    "            layer.W.initializer.run(session=session)\n",
    "            layer.b.initializer.run(session=session)\n",
    "        if isinstance(layer, Conv2D):\n",
    "            layer.W.initializer.run(session=session)\n",
    "            layer.b.initializer.run(session=session)\n",
    "        else:\n",
    "            print(layer, \"not reinitialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=model_pcg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pcg only evaluation\n",
      "WARNING:tensorflow:From <ipython-input-12-6d4d43a38123>:19: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-12-6d4d43a38123>:19: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 90 steps, validate for 22 steps\n",
      "Epoch 1/15\n",
      "89/90 [============================>.] - ETA: 1s - loss: 0.7062 - categorical_accuracy: 0.7449"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'sparse_categorical_accuracy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-6d4d43a38123>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mPARAMS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'epochs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         callbacks=[\n\u001b[0;32m---> 19\u001b[0;31m                     tf.keras.callbacks.LambdaCallback(on_epoch_end=lambda epoch, logs: log_epoch_data(logs, exp)),]\n\u001b[0m\u001b[1;32m     20\u001b[0m     )\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/core-env/lib/python3.6/site-packages/tensorflow_core/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m               \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m               instructions)\n\u001b[0;32m--> 324\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m     return tf_decorator.make_decorator(\n\u001b[1;32m    326\u001b[0m         \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'deprecated'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/core-env/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1304\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1305\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m   @deprecation.deprecated(\n",
      "\u001b[0;32m~/core-env/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/core-env/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    395\u001b[0m                       total_epochs=1)\n\u001b[1;32m    396\u001b[0m                   cbks.make_logs(model, epoch_logs, eval_result, ModeKeys.TEST,\n\u001b[0;32m--> 397\u001b[0;31m                                  prefix='val_')\n\u001b[0m\u001b[1;32m    398\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/core-env/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mon_epoch\u001b[0;34m(self, epoch, mode)\u001b[0m\n\u001b[1;32m    769\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m         \u001b[0;31m# Epochs only apply to `fit`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 771\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    772\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    773\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/core-env/lib/python3.6/site-packages/tensorflow_core/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m    300\u001b[0m     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m       \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-6d4d43a38123>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(epoch, logs)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mPARAMS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'epochs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         callbacks=[\n\u001b[0;32m---> 19\u001b[0;31m                     tf.keras.callbacks.LambdaCallback(on_epoch_end=lambda epoch, logs: log_epoch_data(logs, exp)),]\n\u001b[0m\u001b[1;32m     20\u001b[0m     )\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/intelliscope/notebooks/physionet2016/pcg_ecg/Ramith/regular/ml_utils.py\u001b[0m in \u001b[0;36mlog_epoch_data\u001b[0;34m(logs, npt)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mlog_epoch_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnpt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mnpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'epoch/sparse_categorical_accuracy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sparse_categorical_accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mnpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'epoch/loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mnpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'epoch/val_sparse_categorical_accuracy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_sparse_categorical_accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'sparse_categorical_accuracy'"
     ]
    }
   ],
   "source": [
    "\n",
    "model.compile(optimizer=keras.optimizers.Adam(),  # Optimizer\n",
    "              # Loss function to minimize\n",
    "              loss=keras.losses.CategoricalCrossentropy(),\n",
    "              # List of metrics to monitor\n",
    "              metrics=[keras.metrics.CategoricalAccuracy()])\n",
    "model.summary(print_fn=lambda x: neptune.log_text('model_summary', x))\n",
    "\n",
    "\n",
    "\n",
    "if model==model_pcg:\n",
    "    print('pcg only evaluation')\n",
    "    model.fit_generator(\n",
    "        pcg_training,\n",
    "        steps_per_epoch = 1803 // batch_size,\n",
    "        validation_data = pcg_validation,\n",
    "        validation_steps = 451 // batch_size,\n",
    "        epochs =PARAMS['epochs'],\n",
    "        callbacks=[\n",
    "                    tf.keras.callbacks.LambdaCallback(on_epoch_end=lambda epoch, logs: log_epoch_data(logs, exp)),]\n",
    "    )\n",
    "    \n",
    "if model==model_ecg:\n",
    "    print('pcg only evaluation')\n",
    "    model.fit_generator(\n",
    "        ecg_training,\n",
    "        steps_per_epoch = 1803 // batch_size,\n",
    "        validation_data = ecg_validation,\n",
    "        validation_steps = 451 // batch_size,\n",
    "        epochs =PARAMS['epochs'],\n",
    "        callbacks=[\n",
    "                    tf.keras.callbacks.LambdaCallback(on_epoch_end=lambda epoch, logs: log_epoch_data(logs, exp)),]\n",
    "    )\n",
    "    \n",
    "if model==model_hybrid:\n",
    "    print('pcg only evaluation')\n",
    "    model.fit_generator(\n",
    "        dual_training,\n",
    "        steps_per_epoch = 1803 // batch_size,\n",
    "        validation_data = dual_validation,\n",
    "        validation_steps = 451 // batch_size,\n",
    "        epochs =PARAMS['epochs'],\n",
    "        callbacks=[\n",
    "                    tf.keras.callbacks.LambdaCallback(on_epoch_end=lambda epoch, logs: log_epoch_data(logs, exp)),]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot(history.history['sparse_categorical_accuracy'], label='sparse_categorical_accuracy')\n",
    "plt.plot(history.history['val_sparse_categorical_accuracy'], label = 'val_sparse_categorical_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0.5, 1])\n",
    "plt.legend(loc='lower right')\n",
    "test_loss, test_acc = model.evaluate([x_test1,resized_x_test2],  y_test, verbose=2)\n",
    "\n",
    "fig.canvas.draw()\n",
    "curve = np.fromstring(fig.canvas.tostring_rgb(), dtype=np.uint8, sep='')\n",
    "curve = curve.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "\n",
    "neptune.log_image('learning_curve', curve)\n",
    "neptune.log_metric('Test Accuracy',test_acc)\n",
    "neptune.log_metric('Test Loss',test_loss)\n",
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loss, acc = model.evaluate([x_test1,resized_x_test2],  y_test, verbose=2)\n",
    "print('Restored model, accuracy: {:5.2f}%'.format(100*acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_model_path = \"/home/ec2-user/SageMaker/notebooks_ramith/out/\"  + str(exp) + '.h5'  \n",
    "\n",
    "model.save(keras_model_path)  # save() should be called out of strategy scope\n",
    "\n",
    "neptune.log_artifact(keras_model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "632/632 [==============================] - 6s 9ms/sample\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.42      0.15      0.23       188\n",
      "           1       0.72      0.91      0.80       444\n",
      "\n",
      "    accuracy                           0.69       632\n",
      "   macro avg       0.57      0.53      0.51       632\n",
      "weighted avg       0.63      0.69      0.63       632\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred = model.predict([x_test1,resized_x_test2], batch_size=100, verbose=1)\n",
    "y_pred_bool = np.argmax(y_pred, axis=1)\n",
    "\n",
    "z = classification_report(y_test, y_pred_bool)\n",
    "print(z)\n",
    "\n",
    "neptune.log_text('classification_report', z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install neptune-contrib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.2.4 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/tensorflow_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-plot -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neptunecontrib.monitoring.metrics import *\n",
    "\n",
    "threshold = 0.5\n",
    "\n",
    "log_confusion_matrix(y_test, y_pred[:, 1] > threshold)\n",
    "log_classification_report(y_test, y_pred[:, 1] > threshold)\n",
    "log_class_metrics(y_test, y_pred[:, 1] > threshold)\n",
    "log_class_metrics_by_threshold(y_test, y_pred[:, 1])\n",
    "log_roc_auc(y_test, y_pred)\n",
    "log_precision_recall_auc(y_test, y_pred)\n",
    "log_brier_loss(y_test, y_pred[:, 1])\n",
    "log_log_loss(y_test, y_pred)\n",
    "log_ks_statistic(y_test, y_pred)\n",
    "log_cumulative_gain(y_test, y_pred)\n",
    "log_lift_curve(y_test, y_pred)\n",
    "log_prediction_distribution(y_test, y_pred[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "neptune.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "neptune": {
   "notebookId": "8952f360-fb9e-4bfb-9709-f109ed7e2c09"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
